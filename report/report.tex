\documentclass[11pt]{report}

\usepackage[toc,page]{appendix}
% CPS equations
\usepackage{amsmath}
\usepackage{stmaryrd}
% Grammar
\usepackage{syntax}
% Operational semantics
\usepackage{semantic}
% Stages diagram
\usepackage{tikz}
% Stages diagram elements
\usepackage{graphicx}
% Chapter number and name on one line
\usepackage{titlesec}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\thechapter}{1em}{} 
% No new page after chapter
%\usepackage{etoolbox}
%\makeatletter
%\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
%\makeatother
% Source code listings
\usepackage{listings}
% Line spacing
\usepackage{setspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{LJSP - A LISP to asm.js compiler}
\author{Jan S\"ondermann}
\date{1st January 1900}

% Commands to make CPS equations easier to write
\newcommand{\eqdef}{\stackrel{\text{def}}{=}}%
\newcommand{\cpstrans}[1]{\ensuremath{\mathcal{K}\llbracket #1 \rrbracket}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% TODO why scala? why LLVM IR/C
% TODO mention that LJSP is a subset of Scheme



\maketitle

\onehalfspacing

\begin{center}
\textbf{Originality avowal}
\end{center}

I verify that I am the sole author of this report, except where explicitly stated to the contrary.

I grant the right to King's College London to make paper and electronic copies of the submitted work for purposes of marking, plagiarism detection and archival, and to upload a copy of the work to Turnitin or another trusted plagiarism detection service.

\begin{flushright}
Jan SÃ¶ndermann \\
@@date@@@
\end{flushright}
\newpage
			
\begin{center}
\textbf{Abstract}
\end{center}

@@@
\newpage

\begin{center}
\textbf{Acknowledgements}
\end{center}
I would like to thank my supervisor Dr. Christian Urban for supervising my project and for providing me with both guidance and freedom.
\newpage

\tableofcontents
\newpage

\chapter{Introduction}
One of the main trends of today's Internet is the shift from traditionally offline or standalone programs to the web. This phenomenon, often @dubbed@ "Web 2.0"@ajax@ is visible in the success of web applications such as GMail and Facebook @other examples@.

This success of the web has brought with it the rise of the programming language it is build on: JavaScript, the only language that is supported by all major browsers. Statistics, such as the number of repositories created on GitHub (a popular code sharing website) using JavaScript, show this success: in 2013, JavaScript led this list by a substantial margin. @cite

Since JavaScript is no longer confined to being used for simple animation and input-validation tasks, the complexity of web apps has increased tremendously. Modern JavaScript frameworks, such as Ember.js@link or Meteor@link use the full register of functionality the language offers and are @just as complex as traditional web frameworks that use languages such as Ruby or Python.@

Unfortunately, this proliferation of JavaScript has taken the language far beyond the tasks it was initially conceived for @cite. There is a widespread consensus@cite the language has a number of shortcomings. Brendan Eich, the creator of JavaScript, writes\cite{brendeich} about its early history:
\begin{quote}
In April 1995 I joined Netscape in order to "add Scheme to the browser." [...]

So in 10 days in May 1995, I prototyped "Mocha," the code name Marc Andreessen had chosen. [...]

To overcome all doubts, I needed a demo in 10 days. I worked day and night, and consequently made a few language-design mistakes (some recapitulating bad design paths in the evolution of LISP), but I met the deadline and did the demo.
\end{quote}

Criticism of JavaScript has mostly focussed on two areas:
\begin{itemize}
\item Inconsistencies and mistakes in the design of the language. Examples for this often given include the confusion around JavaScript's large number of falsy values (\texttt{0},  \texttt{''}, \texttt{NaN}, \texttt{false}, \texttt{null} and \texttt{undefined}) and its set of reserved keywords, which includes a large number of words not in use by the language but doesn't include \texttt{NaN} and \texttt{undefined}. This makes it necessary to test for \texttt{undefined}ness using \\
\mbox{\texttt{typeof x === 'undefined'}} to avoid comparing against a redefined \texttt{undefined}.
\item Slow execution speed. Writing fast interpreters for JavaScript has been an extraordinary difficult task for browser @manufacturers. @why?@
\end{itemize}

Programmers have responded to the first problem in ways that include limiting themselves to a subset of JavaScript that excludes the inconsistent and badly-designed parts (cf. the very popular "JavaScript: The Good Parts"@cite). Another response has been to create new languages that compile to JavaScript, such as the very popular CoffeeScript @cite, Microsoft's TypeScript and Google's Dart.

The second problem has been partly remedied by a new generation of JavaScript engines spearheaded by Google's V8, released in 2008 as part of Google Chrome. The other browser makers, including Mozilla soon followed by rewriting their own JavaScript engines. These new engines often brought impressive speed gains.

In early 2013, Mozilla released asm.js, a project that claims to take these two approaches to their logical conclusion. It defines an extremely limited, statically typed subset of JavaScript that can be executed very quickly. This subset is intended as compilation targets for high level languages.

This project defines such a language and provides a compiler for it that outputs, among other targets, asm.js. It sets out to explore the performance@

% TODO describe goal, history. also include ray tracer

\chapter{Background}
\section{Technology involved}
\subsection{asm.js}
\subsection{emscripten and LLVM}
% paper (desktop)

% TODO possibly include this at the end if not at word limit yet
%\section{Similar Languages}

\subsection{Ray tracing}


\chapter{Requirements}

\chapter{Specification}
\section{Grammars}
\subsection{LJSP Grammar}
The grammar of LJSP in Extended Backus-Naur Form is given below.
\begin{grammar}
<program> ::= <defines> [ <expr> ] <defines>

<defines> ::= <define> <defines> | $\epsilon$

<define> ::= `(define (' <ident> <params> `)' <expr> `)'

<expr> ::= <double>
\alt <ident>
\alt `(if' <expr> <expr> <expr> `)'
\alt <lambda>
\alt `(let (' <letblocks> `)' <expr> `)'
\alt `(' <primOp> <args> `)'
\alt `(' <expr> <args> `)'

% add scientific form
<double> ::= \texttt{-?(\textbackslash d+(\textbackslash.\textbackslash d*)?|\textbackslash d*\textbackslash.\textbackslash d+)}

<ident> ::= \texttt{[a-zA-Z=*+/\textless\textgreater!?-][a-zA-Z0-9=*+/\textless\textgreater!?-_]*}

<lambda> ::= `(lambda (' <params> `)' <expr> `)'

<params> ::= <ident> | <ident> <params>

<letblocks> ::= <letblock> | <letblock> <letblocks>

<letblock> ::= `(' <ident> <expr> `)'

<primOp> ::= `+' | `-' | `*' | `/' | `neg'
\alt `>=' | `<=' | `=' | `not' | `<' | `>' | `and' | `or'
\alt `min' | `max'
\alt `sqrt'

% is an application to an empty argument list legal?
<args> ::= <expr> | <expr> <args>
\end{grammar}

\subsection{Intermediate Representation (IR) Grammar}
The grammar for expressions used in intermediate stages is as follows:
\begin{grammar}
<expr> ::= <env>
\alt `(make-closure' <lambda> <env> `)'
\alt `(nth' <int> <expr> `)'
\alt `(get-env' <expr> `)'
\alt `(get-proc' <expr> `)'
% should this be <expr> instead of <env>?
\alt `(hoisted-lambda' <ident> <env> `)'

<env> ::= `(make-env' <idents> `)'

<int> ::= \texttt{-?(0|[1-9][0-9]*)}

<idents> ::= <ident> <idents> | $\epsilon$
\end{grammar}

The grammer for the Intermediate Representation of the LJSP compiler is given below.
\begin{grammar}
<iModule> ::= <iFunctions>

<iFunctions> ::= <iFunction> <iFunctions> | $\epsilon$

<iFunction> ::= `function' <ident> `(' <params> `)' `{' <iStatements> `}'

<iStatements> ::= <iStatement> <iStatement> | $\epsilon$

<iStatement> ::= <ident> `=' <iExpr>
\alt `if (' <ident> `) {' <iStatements> `} else {' <iStatements> `}'
\alt <iExpr>

<iExpr> ::= <ident>
\alt <double>
\alt <ident> `(' <iArgs> `)'
\alt <iUnaryPrimOp> <iExpr>
\alt <iExpr> <iBinaryPrimOp> <iExpr>
\alt `make-env(' <iIdents> `)'
\alt `make-hl(' <ident> `,' <ident> `)'
\alt <ident> `[' <index> `]'

<iArgs> ::= <iExpr> | <iExpr> <iArgs>

<iUnaryPrimOp> ::= `sqrt'

<iBinaryPrimOp> ::= `+' | `-' | `*' | `/'
\alt | `==' | `<' | `>'
\alt `min' | `max'

<iIdents> ::= <ident> | <ident> <iIdents>

<index> ::= \texttt{0 | [1-9][0-9]*}


\end{grammar}


% IR grammar

% grammar of intermediate representation, C?

% subset of scheme
\section{Operational Semantics of LJSP}
\begin{gather*}
%boolean
%idn
\inference[d]{}{d~\Downarrow~d} \\
\inference[if(false)]{e_1~\Downarrow~\text{\#f}\hspace{1cm}e_3~\Downarrow~v}{(\text{if}\ e_1, e_2, e_3)~\Downarrow~v}\\
\inference[if(true)]{e_1~\Downarrow~\text{\#t}\hspace{1cm}e_2~\Downarrow~v}{(\text{if}\ e_1, e_2, e_3)~\Downarrow~v}
\end{gather*}

%\section{Backend Output}

\chapter{Design}
This chapter will describe the design of the LJSP compiler. It will give an abstract, theoretical description of all the subsystems that make up the compiler. The next chapter will then describe the concrete implementation of these subsystems. Together, these two chapters will give the reader a full understanding of the inner workings of the LJSP compiler.
\section{Overall architecture of the Compiler}
The compiler is made up of two major components: A hierarchy of classes that get instantiated to create the Abstract Syntax Tree (AST) and a large number of compilation stages that perform various transformation on this AST and all manipulate it to @varying@ degrees to bring it into a form closer to the desired output.

When compiling a program, the compiler will first create the AST of the input program given. It will then run this AST through a number of stages depending on the desired output (e.g. asm.js or C) before finally converting the resulting AST back to a textual representation.

We will now first describe the AST classes before going through all the individual stages of the compiler in detail.
\subsection{The Abstract Syntax Tree}
% cite: something about ASTs maybe from the dragon book
The Abstract Syntax Tree (AST) is the representation of the program that the compiler uses internally. It gets generated by the first stage, parsing, whose purpose it is to convert the users's textual representation of the program to an AST. Correspondingly, the compiler includes functions that convert an AST back to program code in text form. The details of these conversion will be described in more detail in the implementation chapter.

As its name suggests, the Abstract Syntax Tree is a Tree representation of the code. As an illustration of the structure of these trees, we give the AST for the expression \hbox{\texttt{(* (+ x 1) 2)}} below.
% TODO figure

The AST is closely connected to the LJSP Grammar given in the Specification chapter: Every node in the tree corresponds to one @rule@ in the grammar. @in figure: rule next to node@. This relationship will become more obvious in the next chapter, when we describe the implementation of LJSP's ASTs.

% scheme is used for all internal code
% conversion are neutral/transparent with regards to the expression the proram evaluates to

\subsection{Compilation stages}
Because compiling a program from one programming language to another is often a very complex transformation, it is common to @cite@ break up this transformations into a number of stages that each accomplish a small part of this overall transformation. While these stages can vary quite significantly in complexity ranging from simple expansions of specific expressions (an example would be converting expressions of the form \hbox{\texttt{1 + 2 + 3 + 4}} to \hbox{\texttt{((1 + 2) + 3) + 4}}) to more complex transformations such as CPS-Translation, they are all of managable complexity when compared to the overall goal of transforming from the input to the output language.

% say something about intermediate languages what the stages input/output
% the stages should not influence what it evaluates to

Like many other compilers, the LJSP compiler is made up of such stages. These compilation stages make up the core of the compiler and will be described in detail the following subsections.

% nanocompiler

\subsubsection{Overview of compilation stages}

%structure: front end, mid end back end

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.8]{stagesflowchart.eps}
\caption{Flowchart for compilation stages in LJSP}
\end{center}
\label{stagesflowchart}
\end{figure}

% TODO the name underneath the figure and the one in the text don't match
Figure \ref{stagesflowchart} shows all the compilation stages that the LJSP compiler is made up of. These stages  can be grouped into three sections:
\begin{itemize}
\item \textbf{Front end stages}: These first six stages make up the first part of the transformations the code passes through. Their structure is that of a simple list as there is no branching of stages in the front end. These stages all operate on LJSP code, but they all reduce the complexity of the code. Letn expansion and prim op reduction reduce the number of distinct instructions in the code, CPS-translation makes control flow explicit while the latter two stages remove anonymous function objects by turning them into named functions. This reduction of complexity is necessary to bridge the gap between LJSP, a very high-level language that includes such concepts as functions as first class variables, and low-level output languages such as LLVM IR that do not include these concepts. In the diagram, they are depicted as boxes with rounded edges.
% reduce complexity: remove lambdas, reduce certain prim ops rounded edges.
\item \textbf{Intermediate stages}: These stages convert the LJSP AST obtained after passing through the front end stages into a different language called the Intermediate Representation (IR) of the LJSP compiler before performing various optimisations on this IR code. This part of the compiler currently consists of two stages, one that converts LJSP to IR and a simple optimisation that removes redundant assignments. It offers a suitable framework for adding additional optimisations that automatically get shared between all output languages. In the diagram, intermediate stages are shown as plain boxes without rounding or background colour.

% TODO put this somewhere else: IR is an imperative, untyped language, that is general enough to be converted to both asm.js and C but close enough to these languages to avoid code duplication. 
% HIST: later addition, before hoist straight to asm.js later hoist -> IR/asm.js: code duplication

\item \textbf{Back end stages}: This part of the compiler comprises all the stages that result in an output language. The relationships between these stages are more complex and include some branching, the reasons for which will be described in the section below. In the diagram, back end stages are shown as boxes with a gray background.
\end{itemize}

This subdivision of stages into the three groups described above has been inspired by large, industrial strength compilers such as LLVM.
% TODO @include LLVM stages diagram@
% TODO move description of stages in LLVM and **Nano stage compiler** somewhere else, maybe background

\section{Detailed description of compilation stages}
We will now describe all the stages of the LJSP compiler in detail. For many stages, this will include a set of equations to define the stages in a precise way. In these equations, $y$ will stands for a variable, $d$ for a double constant and $f$ for a fresh identifier. Functions of the form $\mathcal{A}\llbracket e \rrbracket$ will denote transformation $\mathcal{A}$ applied to expression $e$.

\subsection{Parsing}
This very first stage takes the textual representation of the program and converts it to an Abstract Syntax Tree that the subsequent stages manipulate. It does so according to the grammar of LJSP described in the Specification chapter. Because of its syntax, LJSP is a very easy language to parse, which makes this a relatively simple stage. The details of parsing will be described in the Implementation chapter.

\subsection{\texttt{letn} expansion}
\begin{figure}[ht]
\begin{alignat*}{2}
&\mathcal{L}\llbracket (\text{let}\ ((idn_1\ e_1)\ (idn_2\ e_2)\dots &&\eqdef (\text{let}\ ((idn_1\ e_1))\ (\text{let}\ ((idn_2\ e_2)) \dots \\
&\hspace{1cm} (idn_n\ e_n))\ e)\rrbracket &&\hspace{1cm}(\text{let}\ ((idn_n\ e_n))\ e)))\\
&\mathcal{L}\llbracket e \rrbracket && \eqdef e\text{ with $\mathcal{L}$ applied to all expressions in e}\\
\end{alignat*}
\caption{\texttt{letn} conversion}
\label{letnconversion}
\end{figure}
This is a small stage that expands all \texttt{let} expressions that define $n$ @variables@ into $n$ \texttt{let} expressions that all define one variable each. Applying \texttt{letn} conversion to the expression \texttt{(let ((a 1) (b 2)) (+ a b))} would yield \texttt{(let ((a 1)) (let ((b 2)) (+ a b)))}. While not strictly necessary, this conversion greatly simplifies later stages such as CPS-translation, as they do not have to consider the possibility of multiple $(idn\ e)$ blocks inside \texttt{lets}. Equations for this stage are given in figure \ref{letnconversion}.

\subsection{Prim op reduction}
\begin{figure}[ht]
\begin{alignat*}{2}
&\mathcal{L}\llbracket (\text{let}\ ((idn_1\ e_1)\ (idn_2\ e_2)\dots &&\eqdef (\text{let}\ ((idn_1\ e_1))\ (\text{let}\ ((idn_2\ e_2)) \dots \\
&\hspace{1cm} (idn_n\ e_n))\ e)\rrbracket &&\hspace{1cm}(\text{let}\ ((idn_n\ e_n))\ e)))\\
&\mathcal{P}\llbracket e \rrbracket && \eqdef e\text{ with $\mathcal{P}$ applied to all expressions in e}\\
\end{alignat*}
\caption{Prim op reduction}
\label{primopreduction}
\end{figure}
This stage reduces a number of primitive operations to a combination of other primitive operations. This reduces work in backend stages, as only as limited subset of primitive operations that are part of the LJSP grammar need to get converted to the respective language that the backend generates. One example a primitive operation that the parser recognises but that backend stages do not have to handle is the negation operator $-$ that switches the sign of a double. Prim op reduction reduces expressions of the form $-x$ to simple subtraction: $0-x$. The other primitive operations that Prim op reduction removes from the AST are given in \ref{primopreduction}
\subsection{CPS-Translation}
This stage converts the program to continuation-passing style (CPS). Because it is the central stage of the compilation pipeline, we will first explain continuation-passing style in general before describing the particularities of the CPS-translation in the LJSP compiler.

% TODO give names to snippets, refer to them by name
% TODO based on system f paper
\subsubsection{Continuation-Passing Style}
Code that is in Continuation-Passing Style gives explicit names to every intermediate computation and makes control flow explicit by never returning from a function call.\cite{sysftal}\cite{appel} To illustrate this, we show the transformation of a simple function \texttt{f}, given below, to CPS step-by-step.

\begin{lstlisting}
function f(a, b) {
  return a + b + 10
}
\end{lstlisting}

First, we give a name to the result of the additions that the function returns:

\begin{lstlisting}
function f1(a, b) {
  t1 = a + b + 10
  return t1
}
\end{lstlisting}

To conform to the requirement that every intermediate result be explicitely named, we need to break up the long sum into two computations. This makes it clear that the first addition gets precedence before the second:

\begin{lstlisting}
function f2(a, b) {
  t2 = a + b
  t1 = t2 + 10
  return t1
}
\end{lstlisting}

The last step in our example is the one that gives Continuation-Passing Style its name: Functions in CPS conform programs do not return, but instead receive a function, usually called "continuation", that the caller passes in an additional parameter. This continuation can be understood as the remainder of the program that processes the result computed by the function:

\begin{lstlisting}
function f3(cont, a, b) {
  t2 = a + b
  t1 = t2 + 10
  cont(t1)
}
\end{lstlisting}

Code that is CPS conform is nearly linear: it consists of a sequence of let expressions followed by a single function call. The only expression that violates this linearity is the if-expression that branches execution flow.

To further illustrate the concept of continuations, we give a short program written in Scheme below. Scheme is unique among most programming languages in that it gives the programmer access to the continuations in his program using a function called \texttt{call/cc}\footnote{call/cc stands for "call with current continuation". Unlike in most other languages, identifiers in Scheme can contain the '/' character.}. \texttt{call/cc} takes as its one parameter a function, which in term gets called with the current continuation as argument. Our example program uses \texttt{call/cc} to save and later reuse a continuation inside an arithmetic operation.

We first define a variable \texttt{cont} that will later hold our saved continuation. Variables in Scheme need to be defined before they can be set! later, right now \texttt{cont} holds a dummy value of 0. We also define a function \texttt{set-cont} that takes a parameter, sets \texttt{cont} to its value and returns 1. This may seem redundant, but it simplifies the next line.
\begin{lstlisting}
> (define (cont) 0)
> (define (set-cont c) (set! cont c) 1)
\end{lstlisting}

Next, we assign our continuation. \texttt{call/cc} gets called with set-cont which in turn gets called with the continuation. The return value of set-cont, which is 1, is the value that gets passed to the arithmetic operations and this line returns 3.
\begin{lstlisting}
> (+ 1 (* 2 (call/cc set-cont)))
3
\end{lstlisting}

We can now call this continuation again with a different value.
\begin{lstlisting}
> (cont 2)
5
> (cont 10)
> 21
\end{lstlisting}

One way of visualising the continuation is to write it as \texttt{(+ 1 (* 2 ...))}. This is the continuation of the code at \texttt{...} Whatever value this code computes, it passes it on to our continuation. It is worth noting that the contination is not just simply a function that multiplies its argument by 2 and adds 1, as the following code illustrates:
\begin{lstlisting}
> (+ 1000 (cont 2))
5
\end{lstlisting}

Calling \texttt{cont} substitutes the continuation that we see in the code above, \texttt{(+ 1000 ...)}, with our saved continuation.

\subsubsection{CPS-Translation in LJSP}
\begin{figure}[ht]
\begin{alignat*}{2}
&\cpstrans{y} k &&\eqdef k(y) \\
%
&\cpstrans{d} k &&\eqdef k(d) \\
%
&\cpstrans{(\text{if}\ e_1\ e_2\ e_3)} k &&\eqdef \cpstrans{e_1} \lambda x.(\text{if}\ x\ \cpstrans{e_2}k\ \cpstrans{e_3}k) \\
%
&\cpstrans{(\text{define}\ (name, p_1, \dots, p_n)\ e)} k &&\eqdef k((\text{define}\ (name, f_{cont}, p_1, \dots, p_n)\ \\
&&&\hspace{1cm}\cpstrans{e}f_{cont})) \\
%
&\cpstrans{(\text{lambda}\ (p_1, \dots, p_n)\ e)} k &&\eqdef k((\text{lambda}\ (f_{cont}, p_1, \dots, p_n)\ \\
&&&\hspace{1cm}\cpstrans{e}f_{cont})) \\
%
&\cpstrans{(proc\ p_1, \dots, p_n)} k &&\eqdef \cpstrans{p_1} \lambda x_1. \dots  \\
&&&\hspace{1 cm}\cpstrans{p_n} \lambda x_n.\\
&&&\hspace{1.5 cm}(proc\ \lambda x_k.k(x_k), x_1, \dots, x_n) \\
%
&\cpstrans{(prim\ p_1, p_2, \dots, p_n)} k &&\eqdef \cpstrans{(prim\ (prim\ (prim\ p_1, p_2)\ p_3) \dots p_n)} k\\
%
&\cpstrans{(prim\ p_1, p_2)} k &&\eqdef \cpstrans{p_1}\lambda x_1. \cpstrans{p_2}\lambda x_2.\\
&&&\hspace{1cm}(\text{let}\ f=(prim\ x_1, x_2)\ \text{in}\ k(f))\\
%
&\cpstrans{(\text{let}\ ((idn\ e_1))\ e_2)} k &&\eqdef \cpstrans{e_1} \lambda x_1. \\
&&&\hspace{1 cm}(\text{let}\ idn = x_1\ \text{in}\ \cpstrans{e_2} \lambda x_2.(k(x_2)))\\
\end{alignat*}
\caption{CPS translations of LJSP expressions}
\label{cpstrans}
\end{figure}

% move this to the beginning of the chapter
Figure~\ref{cpstrans} shows CPS translations for all LJSP expressions. In the equations, $\cpstrans{e} k$ stands for the CPS translation of expression $e$ with continuation $k$.

We will conclude this section on Continuation-Passing Style by describing some of the more complex CPS translation equations in more detail. The transformation for both the \texttt{define} and \texttt{lambda} expressions CPS translate the body of the function with a continuation passed to the function in a new parameter called $f_{cont}$ in the equations and \texttt{cont_n} in the code. This parameter has a correspondence in the translation for proc in the next equation: it is the $\lambda x_k.k(x_k)$ that gets added to the application when translating. The translation for primitive operations consists of two equations in the diagram, one equation to convert operations with more than one operands into multiple operations with two operands each and one equation to convert it to Continuation-Passing Style. Note that unlike the translation of regular function applications, the application of primitive operations does not include the additional continuation parameter just mentioned, as primitive operations are allowed to return.

\subsection{Closure Conversion}
% TODO properly position this figure
\begin{figure}%[ht]
\begin{alignat*}{2}
&\text{FV}(y) &&\eqdef \{y\}\\
&\text{FV}(d) &&\eqdef \emptyset\\
&\text{FV}((\text{if}\ e_1\ e_2\ e_3)) &&\eqdef \text{FV}(e_1) \cup \text{FV}(e_2) \cup \text{FV}(e_3)\\
&\text{FV}((\text{let}\ ((idn\ e_1))\ e_2)) &&\eqdef \text{FV}(e_1) \cup (\text{FV}(e_2) - \{idn\})\\
&\text{FV}((\text{define}\ (name, p_1, \dots, p_n)\ e)) &&\eqdef \text{FV}(e) - \{name\} - \{p_1, \dots, p_n\}\\
&\text{FV}((\text{lambda}\ (p_1, \dots, p_n)\ e)) &&\eqdef \text{FV}(e) - \{p_1, \dots, p_n\}\\
&\text{FV}((proc\ p_1, \dots, p_n)) &&\eqdef (\text{FV}(proc) - \{\text{defined functions}\}) \cup \\
&&&\hspace{1cm}\{f \mid f \in \text{FV}(p)\ \text{with}\ p \in \{p_1, \dots, p_n\}\}\\
&\text{FV}((prim\ p_1, \dots, p_n)) &&\eqdef \{f \mid f \in \text{FV}(p)\ \text{with}\ p \in \{p_1, \dots, p_n\}\}\\
%
&&&\\
%
&\mathcal{C}\llbracket (\text{lambda}\ (p_1, \dots, p_n)\ e) \rrbracket &&\eqdef \text{make-closure}(\\
&&&\hspace{1cm}(\text{lambda}\ (env, p_1, \dots, p_n)\ e_{new}))\\
&&&\hspace{0.625cm}\text{with}\ e_{new} = \\
&&&\hspace{1cm}\text{assign free vars to env elements}\ \cup \\
&&&\hspace{1.5cm}\mathcal{C}\llbracket e \rrbracket\\
%
&\mathcal{C}\llbracket (func\ p_1, \dots, p_n) \rrbracket &&\eqdef (func\  \mathcal{C}\llbracket p_1 \rrbracket, \dots, \mathcal{C}\llbracket p_n \rrbracket)\\
&\mathcal{C}\llbracket (closure\ p_1, \dots, p_n) \rrbracket &&\eqdef (\mathcal{C}\llbracket closure\rrbracket.code\ \mathcal{C}\llbracket closure\rrbracket.env\\\
&&&\hspace{1cm}\mathcal{C}\llbracket p_1\rrbracket, \dots, \mathcal{C}\llbracket p_n\rrbracket)\\
&\mathcal{C}\llbracket e \rrbracket && \eqdef e\text{ with $\mathcal{C}$ applied to all expressions in e}\\
\end{alignat*}
\caption{Closure Conversion}
\label{cloconv}
\end{figure}
Besides CPS-translation, closure conversion is the other core stage in the frontend of the LJSP compiler. The goal of closure conversion, in conjunction with the stage that follows it, is to convert anonymous function objects (often called lambdas) to named top-level functions. This is necessary when compiling languages that allow functions as first-class objects, such as LISP, to languages that do not have that property, such as C.

A first naive approach to solving this problem can be illustrated by the following code snippet:

\begin{lstlisting}
def return_n_squarer():
    return lambda x: x * x
\end{lstlisting}

When called, the function \texttt{return_n_squarer} returns another function that takes one parameter \texttt{x} and returns $x^2$. In trivial cases such as the one given, it is immediately possible to give a name to the anonymous lambda and lift it to the top level. The resulting code would look like this:

\begin{lstlisting}
def func_0(x):
    x * x
    
def return_n_squarer():
    return func_0
\end{lstlisting}

This snippet compiles and produces the same result as the original version.

We will now consider a slightly more complex function that needs to be closure converted:

\begin{lstlisting}
def get_n_adder(n):
    r = lambda x: x + n
    return r
\end{lstlisting}

This function takes a parameter \texttt{n} and returns a function, that takes its own parameter \texttt{x} and returns $x+n$. \texttt{get_n_adder(1)} returns the successor function, \texttt{get_n_adder(7)} returns a function that adds $7$ to its parameter and returns the sum.

If we try the naive approach that we used in the first example we encounter a problem that didn't arise in the example above:

\begin{lstlisting}
def func_1(x):
    x + n   # Error: 'n' is not defined
    
def get_n_adder(n):
    r = func_1
    return r
\end{lstlisting}

This example shows that there is more to anonymous functions than just the code they define. They can also access variables of the environment they are defined inside. This combination of code and environment is commonly called a "closure". Separating the function from its context without taking care of these variable accesses will not produce valid code. Instead we need to make use of an idea that gives this stage its name: Closure conversion.

Before we explain closure conversion in detail, it is necessary to define two concepts that we will make use of in our explanation: free and bound variables. A variable used in a function is called free, when it is not declared inside that function but instead comes from the environment. Conversely, a variable inside a function is bound, when it is declared within the function. These ideas can be explain very concisely using the lambda-calculus. Consider the simple term $M = \lambda x.(x\ y)$. The variable $x$ is bound in $M$, because it is a parameter to the abstraction. $y$, on the other hand, is a free variable and will likely be defined somewhere in the term that $M$ is a part of.

The goal of closure conversion is to turn all free variables in the function to be converted into bound variables. The way this is achieved is by adding an additional parameter to the lambda, usually an array or dictionary, that contains the values of all previously free variables. Before the body of the lambda is exectued, these variables are then bound by assigning them to the value that the envirionment-variable gives for them.

% TODO how to find free vars

If we bring the lambda from the earlier example into this form, we obtain the following result:

\begin{lstlisting}
lambda env_0, x: 
    n = env_0[0]   # make n bound
    x + n
\end{lstlisting}

Apart from the lambda itself, both the code that defines it and the code that calls it must be adjusted. We will first describe changes to be made to the definitions of lambdas before describing how calling them changes.

After closure conversion is complete, the code no longer defines isolated lambdas. Instead it defines closures, that are made up of lambdas (the code) and an environment variable (the data). The layout and creation of this data structure is implementation dependant and will be described in detail in the implementation chapter. In the example below, we will make use of two functions called \texttt{make_closure} and \texttt{make_lambda} that hide away the creation of their respective data structure.

Using these functions and free variable assignment as shown in the last example, the \texttt{get_n_adder} example from the beginning of this chapter would get converted to the following:

\begin{lstlisting}
def get_n_adder(n):
    r = make_closure(lambda env_0, x: 
                        n = env_0[0]
                        x + n, 
                     make_env(n))
    return r
\end{lstlisting}

Whereas the return type of \texttt{get_n_adder} was a lambda that could be called directly, it is now a closure which requires slightly more work to be executed. Assuming the definition of \texttt{get_n_adder} given in the last example, the following example shows how to obtain and use the successor function:

\begin{lstlisting}
c = get_n_adder(1)

# To get successor of 5:
c.code(c.env, 5)
\end{lstlisting}

\subsection{Hoisting}
This is again a relatively simple stage that works in conjunction with the last stage, closure conversion. Hoisting takes closure converted lambdas and turns them from anonymous functions to named, top-level functions. Because after closure conversion, all variables used in these anonymous functions are bound, this is a very simple process.

The last definition of \texttt{get_n_adder} given in the previous chapter would change to the following code after hoisting:

\begin{lstlisting}
def func_0(env_0, x):
    n = env_0[0]
    x + n, 
    
def get_n_adder(n):
    r = make_closure(func_0, make_env(n))
    return r
\end{lstlisting}

\subsection{Conversion to IR (Intermediate Representation)}
@TODOTODO@
\subsection{Redundant Assignment Removal}
This is an optimisation stage that removes instructions from the IR Abstract Syntax Tree without affecting the meaning of the program. It is based on the observation that every variable only gets assigned to once. After being assigned a value, a variable can occur many times on the right hand sight of an assignment, but never on the left hand sight again.

This stage scans the code for simple assignments that assign one variable to another without any additional computations of the form $x = y$. It then replaces every occurence of $x$ by $y$ in the code following the assignment before removing the assignment entirely. To see that this does not change the result of the program being converted, consider the example below:
\begin{lstlisting}
x = 3
y = x
z = y * y
\end{lstlisting}

After redundant assignment removal, the second statement will have been removed and the code will be changed to
\begin{lstlisting}
x = 3
# Removed assignment was here
z = x * x
\end{lstlisting}
which still evaluates to the same value as before.

\subsection{Conversion to C}
\subsection{Conversion to asm.js}
\subsection{Conversion to emscripten C}
\subsection{Conversion to LLVM IR}
\subsection{Conversion to Numbered LLVM IR}

\section{Design of the Ray Tracer}



\chapter{Implementation}
This chapter will describe the implementation details of the LJSP compiler. It builds on and assumes the previous chapter which describes the parts of the compiler from a theoretical perspective.

We will first give an overview of the files in the repository to enable the reader to find the subsystems described in the design chapter in the code. Following this high-level overview, we will describe the implementation details of a number of compilation stages, filling in the details left out in the last chapter. After @@
% TODO finish this

\section{Structure of the Code}
The code of the LJSP compiler follows a clear structure that separates the code into modules and that makes it easy to find related modules. The necessity for this structure became evident quickly during the development of the compiler, as it was often necessary to edit multiple parts of the code simultaneously.

The code is structured as follows:
\begin{itemize}
\item The entry point of the compiler can be found in \texttt{main.scala}. This files contains functionality to parse and dispatch the arguments the LJSP compiler gets called with by calling the appropriate methods to achieve the desired result.
\item All classes that make up the Abstract Syntax tree can be found in \texttt{AST.scala}. We decided against splitting up this file into a separate file for each language used in the LJSP compiler (LJSP, IR, C, etc.) as that would have lead to unnecessary fragmentation.
\item The compilation stages that make up the LJSP compiler can be found in files with file names starting with a number, e.g. \texttt{03_cps_translation.scala}, optionally followed by a letter that gives a hint as to which stages precede and follow it. These files all contain exactly one object with a name equal to the file name without the number (\texttt{object cps_translation} in the example of CPS-translation) which in turn contain all the methods used in conversion. 
\item The functions that convert ASTs back to specific languages can be found in files that have file names starting with \texttt{code_generation_}. There are five such files in total, one each for every programming language that the LJSP compiler converts to or from: LJSP, IR, asm.js, C and LLVM IR.
\item The file \texttt{run_tests.py} contains the testing framework described further down in the Testing chapter. All other files related to testing can be found in the \texttt{test/} folder.
\item The ray tracer and all files related to it reside in the \texttt{ray_tracer/} folder.
\end{itemize}

%code structure, where to find what in the repo
% code_generation_* as opposite of parser

\section{Implemenation of Various Stages}
% parsing: % uses parser combinator, scala standard lib. maybe write down parser in a mathematical way (the parser combinators
% cps translation: two functions, different types
\section{Structure of the Generated Code}
% closure data structure

\section{Implementation details of the Ray Tracer}
% calls different versions of vectorDotProduct & ray... depending on button clicked
% TODO: describe profiling that shows that raySph... is where the ray tracer spends most of its time. (maybe somewhere else)


\section{Problems encountered \& resolved}
% variables that contain functions, ftables generated at compile time
% memory management in asm.js and C
\section{Experimentation \& Optimisation}
% cps translation different optimisations
% array index vars of type int, others of type double
% (array index vars are all generated, user doesn't see them)

% TODO describe testing
% TODO describe ray tracer
% TODO evaluate choice of scala, maybe not in this chapter (evaluation?)

\chapter{Testing}
% fuzzy testing of compilers
This chapter describes the methods that were used to test the different subsystems of the LJSP compiler. While testing is imperative in all software development, it is especially important in compiler development. The reason for this is that it is usually impossible to determine whether or not a result generated by the compiler is correct simply by looking at the generated code. This is different to other areas of programming like web or user interface development, where a large number of bugs can be found by looking 

During the development of LJSP, two means of testing were being used:
\begin{itemize}
\item A custom testing framework written in Python that covers all of the frontend stages and some of the backend stages.
\item A ray tracer written in JavaScript that was used to test generated asm.js code specifically.
\end{itemize}
The following two sections will describe both systems in detail.

% IR code doesn't get tested because it was a late addition to the compiler and there is only one simple optimisation so far. if development on LJSP continues, an interpreter for IR code will have to be written

\section{Testing using \texttt{run_tests.py}}
% based on the fact described in the design chapter that stages should not affect the result that the program evaluates to
Early on in the development of LJSP, it became necessary to test the output of every front end stage 
\section{Testing using the ray tracer}
% because asm.js evaluates to the same result regardless of
% whether or not it's compiled according to asm.js specification,
% it's possible to add invalid statements and still test the module

% ray tracer to test performance

\chapter{Evaluation}
% ast transformations would be nicer if the compiler generated the subtrees
% from short code snippets instead of creating the objects manually
\section{Evaluation against requirements}
\section{Strengths \& Weaknesses}

\chapter{Conclusions}
\section{Future Work}
\subsection{Implementing a more complete Subset of Scheme}
\subsection{Additional Front Ends}
\subsection{Additional Back Ends}
\subsection{Additional Optimisations}
% IR stages perfect framework for that
\subsection{Better Test Coverage}
% better test coverage
% TODO possibly add: move type inference from C to IR conversion, make IR typed

\newpage

\chapter{Bibliography}
\begin{thebibliography}{1}
  \bibitem{sysftal} Morrisett, Greg and Walker, David and Crary, Karl and Glew, Neal (1999). {\em From System F to Typed Assembly Language}, ACM Trans. Program. Lang. Syst.
    \bibitem{brendeich} Foreword by Brendan Eich to {\em Node: Up and Running}. Available at http://chimera.labs.oreilly.com/books/1234000001808/pr02.html
    \bibitem{appel} Andrew Appel (1992). {\em Compiling with Continuations} Cambridge University Press
\end{thebibliography}


\begin{appendices}

\chapter{User Guide}
\section{Compiler}
\section{Ray Tracer}
\section{Testing}

\chapter{Programs Listings}

\end{appendices}

\end{document}
